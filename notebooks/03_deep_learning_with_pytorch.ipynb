{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 3 | Deep Learning with PyTorch\n",
    "\n"
   ],
   "id": "1ddc3adfd89545e2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Tensors\n",
    "\n",
    "### The Creation of Tensors\n"
   ],
   "id": "e0e7363f0a109543"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T10:51:18.827166Z",
     "start_time": "2025-10-29T10:51:17.914621Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch as th\n",
    "import numpy as np\n",
    "\n",
    "a = th.FloatTensor(3, 2)\n",
    "a"
   ],
   "id": "ac99f377b294f8a0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The script import PyTorch and NumPy a d create a new float tensor of size 3×2.\n",
    "PyTorch now initialises memory with zeros, a different behaviour from previous versions.\n",
    "Originally it just allocated memory and kapt it uninitialised: faster but less safe.\n",
    "This behaviour might change again in the future so it is good practice to always initialise tensors:"
   ],
   "id": "80ecced3b762d79f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T10:52:35.225451Z",
     "start_time": "2025-10-29T10:52:35.221592Z"
    }
   },
   "cell_type": "code",
   "source": "th.zeros(3, 4)",
   "id": "39edc8ca41598524",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Alternatively call the tensor modification method:",
   "id": "f1dbfb0137b78241"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T10:52:15.872738Z",
     "start_time": "2025-10-29T10:52:15.866913Z"
    }
   },
   "cell_type": "code",
   "source": "th.zero_(a)",
   "id": "2e59db004efc67a8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T10:53:40.133665Z",
     "start_time": "2025-10-29T10:53:40.131798Z"
    }
   },
   "cell_type": "markdown",
   "source": [
    "Notice that there are two types of operation for tensors: **inplace** and **functional**.\n",
    "Inplace operations have an underscore appended to their name, operate on the tensor's content, and eventually return the modified tensor itself.\n",
    "The functional equivalent creates a copy of the tensor's content, applies the operation and returns the copy leaving the original tensor untouched.\n",
    "Inplace operations are usually faster but less safe (the same tensor shared in different places where changes are not expected)."
   ],
   "id": "9b8934c35467ef4a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T10:59:41.303512Z",
     "start_time": "2025-10-29T10:59:41.298868Z"
    }
   },
   "cell_type": "code",
   "source": "th.FloatTensor([[1,2,3],[3,2,1]])",
   "id": "5d0b03c095b83de2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [3., 2., 1.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T10:59:41.303512Z",
     "start_time": "2025-10-29T10:59:41.298868Z"
    }
   },
   "cell_type": "markdown",
   "source": "Alternatively use the tensor constructor passing a Python iterable (i.e.: a list or tuple) to it:",
   "id": "4c154242151f2442"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T11:00:06.941989Z",
     "start_time": "2025-10-29T11:00:06.935556Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n = np.zeros(shape=(3, 2))\n",
    "n\n"
   ],
   "id": "8abf4be975f9a380",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T11:00:24.440523Z",
     "start_time": "2025-10-29T11:00:24.431480Z"
    }
   },
   "cell_type": "markdown",
   "source": "or by passing a NumPy array to it:",
   "id": "4d74f47fc690c109"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T11:03:32.393029Z",
     "start_time": "2025-10-29T11:03:32.387764Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n = np.zeros(shape=(3, 2))\n",
    "b = th.tensor(n)\n",
    "b\n"
   ],
   "id": "b975dd29758c9e3c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T10:53:40.789251Z",
     "start_time": "2025-10-29T10:53:40.787502Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "9cc6043e6492931c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T12:30:32.753184Z",
     "start_time": "2025-10-29T12:30:32.749946Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "21e0c217ca0cc854",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T12:30:32.898499Z",
     "start_time": "2025-10-29T12:30:32.897212Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "5a403150939746ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T12:30:33.054371Z",
     "start_time": "2025-10-29T12:30:33.053026Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "6f7dd013cdbaa21a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T12:30:33.203333Z",
     "start_time": "2025-10-29T12:30:33.201861Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "96475fd4749d541c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T12:30:33.346193Z",
     "start_time": "2025-10-29T12:30:33.344611Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "4cc091c35ea32e6f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T12:30:33.490104Z",
     "start_time": "2025-10-29T12:30:33.488645Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "71032dc31333a5c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T12:30:33.659586Z",
     "start_time": "2025-10-29T12:30:33.657716Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "80a2d2be7e13ef95",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T12:30:33.825565Z",
     "start_time": "2025-10-29T12:30:33.823667Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "f8e0081b0909c517",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Computing a gradient in deep learning means calculating how much a model’s output (typically a loss) changes with respect to each model parameter (weights and biases). Formally, it’s the vector of partial derivatives of the loss with respect to the parameters. These gradients tell us the direction and magnitude to adjust parameters to reduce the loss during training. In practice, frameworks use automatic differentiation (backpropagation) to compute these efficiently.\n",
    "\n",
    "Key ideas:\n",
    "- **Loss function**: A scalar measuring model error (e.g., cross-entropy, MSE).\n",
    "- **Parameters**: Trainable tensors (weights/biases) the model learns.\n",
    "- **Gradient**: For each parameter θ, compute ∂Loss/∂θ.\n",
    "- **Backpropagation**: Apply the chain rule from outputs back to inputs/parameters to get all gradients.\n",
    "- **Optimiser step**: Update parameters, e.g., θ ← θ − η ∂Loss/∂θ with learning rate η.\n",
    "\n",
    "Below are concise Python examples showing how gradients are computed and used.\n",
    "\n",
    "PyTorch: automatic differentiation and a simple training step\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Simple linear model: y_hat = xW + b\n",
    "torch.manual_seed(0)\n",
    "model = nn.Linear(in_features=3, out_features=1)  # parameters: weight (1x3), bias (1)\n",
    "criterion = nn.MSELoss()\n",
    "optimiser = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Dummy data\n",
    "x = torch.randn(5, 3)          # 5 samples, 3 features\n",
    "y = torch.randn(5, 1)          # targets\n",
    "\n",
    "# Forward pass: compute predictions and loss (computational graph is built)\n",
    "y_hat = model(x)\n",
    "loss = criterion(y_hat, y)\n",
    "\n",
    "print(\"Loss (forward):\", loss.item())\n",
    "\n",
    "# Backward pass: compute gradients dLoss/dParam for all Params in the graph\n",
    "optimiser.zero_grad()          # clear previous gradients\n",
    "loss.backward()                # backprop: populates .grad fields\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, \"grad shape:\", param.grad.shape)\n",
    "\n",
    "# Parameter update: gradient descent step\n",
    "optimiser.step()\n",
    "```\n",
    "\n",
    "What’s happening:\n",
    "- During the forward pass, PyTorch builds a graph of operations.\n",
    "- `loss.backward()` applies the chain rule to compute gradients for every leaf parameter.\n",
    "- Each parameter tensor gets a `.grad` attribute holding its gradient.\n",
    "- The optimiser updates parameters using these gradients.\n",
    "\n",
    "PyTorch: inspecting a manual gradient for a simple function\n",
    "```python\n",
    "import torch\n",
    "\n",
    "# f(w) = (w^2).mean(), compute df/dw at some w\n",
    "w = torch.tensor([[1.0, -2.0], [0.5, 3.0]], requires_grad=True)\n",
    "f = (w ** 2).mean()\n",
    "f.backward()  # df/dw = (2w)/N where N is number of elements\n",
    "\n",
    "print(\"w:\", w)\n",
    "print(\"f:\", f.item())\n",
    "print(\"df/dw:\", w.grad)  # gradient matches analytic derivative\n",
    "```\n",
    "\n",
    "TensorFlow (Keras): using GradientTape\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "model = tf.keras.Sequential([tf.keras.layers.Dense(1, input_shape=(3,))])\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "optimiser = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "\n",
    "x = tf.random.normal((5, 3))\n",
    "y = tf.random.normal((5, 1))\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    y_hat = model(x, training=True)\n",
    "    loss = loss_fn(y, y_hat)\n",
    "\n",
    "# Compute gradients of loss w.r.t. model trainable variables\n",
    "grads = tape.gradient(loss, model.trainable_variables)\n",
    "for var, g in zip(model.trainable_variables, grads):\n",
    "    print(var.name, \"grad shape:\", g.shape)\n",
    "\n",
    "# Apply gradients (parameter update)\n",
    "optimiser.apply_gradients(zip(grads, model.trainable_variables))\n",
    "```\n",
    "\n",
    "Small, from-scratch example: manual backprop for a single neuron\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# y_hat = ReLU(x @ w + b), loss = 0.5 * (y_hat - y)**2\n",
    "x = np.array([0.2, -0.5, 1.0])\n",
    "y = 0.8\n",
    "w = np.array([0.1, -0.3, 0.5])\n",
    "b = 0.0\n",
    "lr = 0.1\n",
    "\n",
    "# Forward\n",
    "z = x.dot(w) + b\n",
    "y_hat = max(0.0, z)  # ReLU\n",
    "loss = 0.5 * (y_hat - y)**2\n",
    "\n",
    "# Backward (chain rule)\n",
    "dL_dyhat = (y_hat - y)\n",
    "dyhat_dz = 1.0 if z > 0 else 0.0\n",
    "dL_dz = dL_dyhat * dyhat_dz\n",
    "dL_dw = dL_dz * x          # vector, same shape as w\n",
    "dL_db = dL_dz\n",
    "\n",
    "# Update\n",
    "w -= lr * dL_dw\n",
    "b -= lr * dL_db\n",
    "```\n",
    "\n",
    "Common practical points:\n",
    "- **Retaining graphs**: In PyTorch, the computation graph is freed after `backward()` unless `retain_graph=True`, which you rarely need for standard training loops.\n",
    "- **No grad mode**: Wrap evaluation/inference in `torch.no_grad()` or `tf.inference_mode()` to skip gradient tracking for speed and memory.\n",
    "- **Exploding/vanishing gradients**: Use normalisation, residual connections, appropriate initialisation, or gradient clipping.\n",
    "- **Non-scalar losses**: Reduce to a scalar (e.g., mean) before calling `backward()`; PyTorch allows a vector-Jacobian product if you pass `gradient=...`.\n"
   ],
   "id": "9c992ba776ceb8f7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T12:31:24.177680Z",
     "start_time": "2025-10-29T12:31:24.176065Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "9ab52a9a9503ad95",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T12:31:24.374084Z",
     "start_time": "2025-10-29T12:31:24.372564Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "6c9ceef24018b979",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T12:31:24.569802Z",
     "start_time": "2025-10-29T12:31:24.568291Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "168dee19d563413",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## GAN on Atari Images\n",
    "\n",
    "Instead of relying on the MNIST dataset to show the power of DL, this section will use Generative Adversarial Networks (GANs) to generate screenshots of various Atari games.\n",
    "\n",
    "Generative Adversarial Networks (GANs) use two competing neural networks.\n",
    "One is a **generator** that creates synthetic data, a _\"cheater\"_.\n",
    "The other is a **discriminator** that tries to tell real data from fake, a _\"detective\"_.\n",
    "Through this adversarial process, the generator learns to produce increasingly realistic samples.\n",
    "Conversely, the discriminator becomes better at spotting fakes.\n",
    "Over time, both models improve by challenging each other.\n",
    "\n",
    "GANs are used to improve image quality, generate realistic images, and support feature learning for downstream tasks.\n",
    "In this case, there is practical value if not reinforcing what we have learned so far on PyTorch.\n"
   ],
   "id": "f7e80cddf27d592"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T13:07:39.519079Z",
     "start_time": "2025-10-29T13:07:38.987648Z"
    }
   },
   "cell_type": "code",
   "source": "import gymnasium as gym",
   "id": "8b4efd3e4784f41e",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T13:08:38.153088Z",
     "start_time": "2025-10-29T13:08:38.149348Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class InputWrapper(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Preprocessing of input numpy array:\n",
    "    1. resize image into predefined size\n",
    "    2. move color channel axis to a first place\n",
    "    \"\"\"\n",
    "    def __init__(self, *args):\n",
    "        super(InputWrapper, self).__init__(*args)\n",
    "        old_space = self.observation_space\n",
    "        assert isinstance(old_space, spaces.Box)\n",
    "\n",
    "        self.observation_space = spaces.Box(\n",
    "            self.observation(old_space.low),\n",
    "            self.observation(old_space.high),\n",
    "            dtype=np.float32\n",
    "        )"
   ],
   "id": "712fff3e8c9628de",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9cd3b98af12b0b0d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
